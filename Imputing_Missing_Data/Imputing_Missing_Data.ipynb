{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Imputing_Missing_Data.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO3vHd/dIl3MEcd/vXUiubw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilal-rachik/Feature_Engineering/blob/main/Imputing_Missing_Data/Imputing_Missing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOhXeKtXh9le"
      },
      "source": [
        "# Imputing Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWNe_4N7iCC_"
      },
      "source": [
        "Les données manquantes font référence à l'absence de valeurs pour certaines observations et constituent un problème inévitable dans la plupart des sources de données. Scikit-learn ne prend pas en charge les valeurs manquantes en entrée, nous devions donc supprimer les observations avec des données manquantes ou les transformer en valeurs autorisées. Le faite de remplacer les données manquantes par des estimations statistiques est appelé imputation. L'objectif de toute technique d'imputation est de produire un ensemble de données complet qui peut être utilisé pour former des modèles d'apprentissage automatique. Il existe plusieurs techniques d'imputation que nous pouvons appliquer à nos données. Le choix de la technique d'imputation que nous utilisons dépendra du fait que les données manquent soit hasard, et du nombre de valeurs manquantes et du modèle d'apprentissage automatique que nous avons l'intention d'utiliser. Dans cet article, nous discuterons de plusieurs techniques d'imputation des données manquantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fQqnYjmh8J-"
      },
      "source": [
        "Cet article couvrira les points suivants :\n",
        "\n",
        "* Suppression des observations contenant des données manquantes\n",
        "* Effectuer une imputation moyenne ou médiane\n",
        "* imputation par mode ou par catégorie fréquente\n",
        "* Remplacer les valeurs manquantes par un nombre arbitraire\n",
        "* Imputation par catégorie manquante\n",
        "* Ajout d'une variable indicatrice manquante\n",
        "* Effectuer une imputation multivariée avec MICE, KNN ou Random Forests\n",
        "* Assembler un pipeline d'imputation avec scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmEP8vz3mn9U"
      },
      "source": [
        "Dans cet article, nous utiliserons les librairies Python : pandas, NumPy et scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q03OPXi1n8a_"
      },
      "source": [
        "Nous utiliserons également l'ensemble de données d'approbation de crédit, qui est disponible dans le référentiel UCI MachineLearning (https://archive.ics.uci.edu/ml/datasets/credit+approval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUHsPKYxpABY"
      },
      "source": [
        "Pour préparer l'ensemble de données, procédez comme suit :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daYRdY-1mbzF"
      },
      "source": [
        "#Importez les bibliothèques Python requises :\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye-5CpWfnMHw"
      },
      "source": [
        "#Chargez les données avec la commande suivante\n",
        "data = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU8rr9VYnW-X"
      },
      "source": [
        "#Créez une liste avec des noms de variables :\n",
        "varnames = ['A'+str(s) for s in range(1,17)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9CzAM9hpCGi"
      },
      "source": [
        "#Ajoutez les noms de variables au dataframe :\n",
        "data.columns = varnames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvOuEf4WpYKP"
      },
      "source": [
        "#Remplacez les points d'interrogation (?) dans l'ensemble de données par les valeurs NumPy NaN\n",
        "data = data.replace('?', np.nan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpNOHuJqpooJ"
      },
      "source": [
        "#changer le type des variables numériques en types de données flottants :\n",
        "data['A2'] = data['A2'].astype('float')\n",
        "data['A14'] = data['A14'].astype('float')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8agQ5Kbp5QR"
      },
      "source": [
        "#Recoder la variable cible en binaire :\n",
        "data['A16'] = data['A16'].map({'+':1, '-':0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WupmjFmaqUVI"
      },
      "source": [
        "#Ajoutez des valeurs manquantes à des positions aléatoires dans quatre variables :\n",
        "random.seed(9001)\n",
        "values = set([random.randint(0, len(data)) for p in range(0, 100)])\n",
        "for var in ['A3', 'A8', 'A9', 'A10']:\n",
        "  data.loc[values, var] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1gWPxlXq7Dw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1qn4YpcrN6h"
      },
      "source": [
        "##Suppression des observations avec des données manquantes\n",
        "L'analyse complète de cas (CCA)(Complete Case Analysis) , consiste à rejeter les observations pour lesquelles les valeurs de l'une des variables sont manquantes. L'ACC peut être appliquée aux variables catégorielles et numériques. L'ACC est rapide et facile à mettre en œuvre et a l'avantage de préserver la distribution des variables, à condition que les données soient manquantes au hasard et que seule une petite proportion des données manque. Cependant, si des données sont manquantes pour de nombreuses variables, la CCA peut entraîner la suppression d'une grande partie de l'ensemble de données.\n",
        "\n",
        "### Comment faire..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHi2rUY9sNop"
      },
      "source": [
        "#Calculons le pourcentage de valeurs manquantes pour chaque variable et trions-les par ordre croissant :\n",
        "data.isnull().mean().sort_values(ascending=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcjeSjZetDUZ"
      },
      "source": [
        "#Maintenant, nous allons supprimer les observations avec des données manquantes dans toute les variables :\n",
        "data_cca = data.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffOqWh8eue40"
      },
      "source": [
        "Pour supprimer des observations où des données sont manquantes dans un sous-ensemble de variables, nous pouvons exécuter data.dropna(subset=['A3', 'A4']). Pour supprimer les observations si des données sont manquantes dans toutes les variables, nous pouvons exécuterata.dropna(how='all')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjnh4iDDs_up"
      },
      "source": [
        "#comparons la taille des ensembles de données de cas d'origine et complet :\n",
        "print('Number of total observations: {}'.format(len(data)))\n",
        "print('Number of observations with complete cases:{}'.format(len(data_cca)))\n",
        "#Ici, nous avons supprimé plus de 100 observations avec des données manquantes, comme indiqué dans la sortie suivante"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6-cS8LFs_c3"
      },
      "source": [
        "## Effectuer une imputation moyenne ou médiane\n",
        "L'imputation moyenne ou médiane consiste à remplacer les valeurs manquantes par la variable moyenne ou médiane. Ceci ne peut être effectué que dans des variables numériques. La moyenne ou la médiane est calculée à l'aide d'un ensemble d'apprentissage, et ces valeurs sont utilisées pour imputer les données manquantes dans les ensembles d'apprentissage et de test, ainsi que dans les données futures que nous avons l'intention de scorer avec le modèle d'apprentissage automatique. Par conséquent, nous devons stocker ces valeurs moyennes et médianes. Les transformateurs Scikit-learn apprennent les paramètres sur l'ensemble d'apprentissage et stockent ces paramètres pour une utilisation future. Ainsi, dans cette recette, nous allons apprendre à effectuer une imputation moyenne ou médiane à l'aide des bibliothèques et pandas thescikit-learn.\n",
        "\n",
        "Remarque : Utiliser l'imputation moyenne si les variables sont distribuées normalement et l'imputation médiane sinon. L'imputation moyenne et médiane peut fausser la distribution des variables d'origine s'il y a un pourcentage élevé de données manquantes\n",
        "\n",
        "Comment faire... :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqgFwGYj1nkj"
      },
      "source": [
        "#Tout d'abord, nous allons importer les pandas et les fonctions et classes requises depuis scikit-learn :\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c07HXAY02dyV"
      },
      "source": [
        "Dans l'imputation moyenne et médiane, les valeurs moyennes ou médianes doivent être calculées à l'aide des variables de l'ensemble d'apprentissage ; Par conséquent, séparons les données en ensembles d'apprentissage et de test et leurs cibles respectives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i46W4ir3EAU"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-okMInpm3m2r"
      },
      "source": [
        "#Vérifions le pourcentage de valeurs manquantes dans l'ensemble d'apprentissage :\n",
        "print(X_train.isnull().mean())\n",
        "#La sortie suivante affiche le pourcentage de valeurs manquantes pour chaque variable :"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnpg9_kYLf3H"
      },
      "source": [
        "### Mean / median imputation with pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZESzMnx4hs1"
      },
      "source": [
        "#Remplacez les valeurs manquantes par la médiane des cinq variables numériques. en utilisant des pandas :\n",
        "for var in ['A2', 'A3', 'A8', 'A11', 'A15']:\n",
        "  value = X_train[var].median()\n",
        "  X_train[var] = X_train[var].fillna(value)\n",
        "  X_test[var] = X_test[var].fillna(value)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4NBqKYl5h5S"
      },
      "source": [
        "Remarque: \n",
        "Le fillna() des pandas renvoie un nouvel ensemble de données avec des valeurs imputées par défaut. Nous pouvons définir l'argument inplace sur True pour remplacer les données manquantes dans le cadre de données d'origine : X_train[var].fillna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt8utE2lLTBf"
      },
      "source": [
        "### Mean / median imputation with Scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2j9BC9k7Rql"
      },
      "source": [
        "Maintenant, imputons les valeurs manquantes par la médiane à l'aide de scikit-learn afin que nous puissions stocker les paramètres appris"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-f8e1i_7p2y"
      },
      "source": [
        "#Pour ce faire, séparons l'ensemble de données d'origine en ensembles d'apprentissage et de test, en ne conservant que les variables numériques :\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['A2', 'A3', 'A8', 'A11', 'A15']], data['A16'], test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA86ezwH78qD"
      },
      "source": [
        "SimpleImputer() de scikit-learn imputera toutes les variables de l'ensemble de données. Par conséquent, si nous utilisons une imputation moyenne ou médiane et que l'ensemble de données contient des variables catégorielles, nous obtiendrons une erreur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ4-Mroz7-H2"
      },
      "source": [
        "#Créons un transformateur d'imputation médian à l'aide de SimpleImputer() de scikit-learn :\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "#Pour effectuer une imputation moyenne, nous devons définir la stratégie sur Mean:\n",
        "#imputer = SimpleImputer(strategy = 'mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OelABn7j8m51"
      },
      "source": [
        "# fit le SimpleImputer() au train set afin qu'il apprenne les valeurs médianes des variables :\n",
        "imputer.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4Q_ZUGJ89cM"
      },
      "source": [
        "#Inspectons les valeurs médianes apprises :\n",
        "imputer.statistics_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acJB1Lqg9N_U"
      },
      "source": [
        "#Remplaçons les valeurs manquantes par des médianes :\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P20AZHlFPS3"
      },
      "source": [
        "### Mean / median imputation with Sklearn selecting features to impute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtGoozQoFTf-"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# to impute missing data with sklearn\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# to split the data sets\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3deEzJwgFWs1"
      },
      "source": [
        "# let's separate into training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qScl2CWkFa2P"
      },
      "source": [
        "\n",
        "# first we need to make a list with the numerical vars\n",
        "numeric_features_mean = ['A2', 'A3', 'A8', 'A11', 'A15']\n",
        "\n",
        "# then we instantiate the imputer within a pipeline\n",
        "numeric_mean_imputer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "])\n",
        "\n",
        "# then we put the features list and the imputer in the column transformer\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('mean_imputer', numeric_mean_imputer, numeric_features_mean)\n",
        "    ], remainder='passthrough')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WBeamQ_FdZW"
      },
      "source": [
        "\n",
        "# now we fit the preprocessor\n",
        "preprocessor.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1RhvJpCFgnO"
      },
      "source": [
        "\n",
        "# and now we impute the data\n",
        "X_train = preprocessor.transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTnrf20-FOrQ"
      },
      "source": [
        "# Note that Scikit-Learn transformers return NumPy arrays!!\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHd2wiFQAC3c"
      },
      "source": [
        "## Imputation par mode ou par catégorie fréquente\n",
        "\n",
        "L'imputation modale consiste à remplacer les valeurs manquantes par le mode. Nous utilisons normalement cette procédure dans les variables catégorielles, d'où le nom  d'imputation de catégorie fréquente. Les catégories fréquentes sont estimées à l'aide de l'ensemble de train, puis utilisées pour imputer des valeurs dans les ensembles de données train, test et futurs. Ainsi, nous devons apprendre et stocker ces paramètres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUe0YmToCkj9"
      },
      "source": [
        "Comment faire...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU06i0IkABUV"
      },
      "source": [
        "#Importons pandas et les fonctions et classes requises depuis scikit-learn :\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61_p1xk_DSb_"
      },
      "source": [
        "#Les catégories fréquentes doivent être calculées à l'aide des variables d'ensemble d'apprentissage, séparons donc les données en ensembles d'apprentissage et de test et leurs cibles respectives :\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ji7PKBfF-3V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByXJbtAOF_ff"
      },
      "source": [
        "### Frequent category imputation with pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHL8_NBkDt0l"
      },
      "source": [
        "#Remplaçons les valeurs manquantes par la catégorie fréquente, c'est-à-dire le mode, pour les quatres variables catégorielles\n",
        "for var in ['A4', 'A5', 'A6', 'A7']:\n",
        "  value = X_train[var].mode()[0]\n",
        "  X_train[var] = X_train[var].fillna(value)\n",
        "  X_test[var] = X_test[var].fillna(value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDf-dWZxGD72"
      },
      "source": [
        "### Frequent category imputation with Scikit-learn\n",
        "Maintenant, imputons les valeurs manquantes par la catégorie la plus fréquente en utilisant scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn1N9zh8GIJg"
      },
      "source": [
        "#Tout d'abord, séparons l'ensemble de données d'origine en ensembles d'apprentissage et de test et ne conservons que les variables catégorielles\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data[['A4', 'A5', 'A6', 'A7']], data['A16'], test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQA78mGJGt1n"
      },
      "source": [
        "# créer un objet d'imputation de catégorie fréquente avec SimpleImputer\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# fit de  l'imputeur sur l'ensembme d'apprentissage pour qu'il apprenne les valeurs les plus fréquentes :\n",
        "imputer.fit(X_train)\n",
        "\n",
        "# on peut regarder les modes appris :\n",
        "imputer.statistics_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl1p0qYRHzEa"
      },
      "source": [
        "# Remplaçons les valeurs manquantes par des catégories fréquentes\n",
        "\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzeV3q0TMvxW"
      },
      "source": [
        "## Remplacer les valeurs manquantes par un nombre arbitraire\n",
        "\n",
        "L'imputation par un nombre arbitraire consiste à remplacer les valeurs manquantes par une valeur arbitraire. Certaines valeurs couramment utilisées incluent 999, 9999 ou -1 pour les distributions positives. Cette méthode convient aux variables numériques.\n",
        "\n",
        "Lors du remplacement des valeurs manquantes par un nombre arbitraire, nous devons faire attention à ne pas sélectionner une valeur proche de la moyenne ou de la médiane, ou toute autre valeur commune de la distribution.\n",
        "\n",
        "Remarque:\n",
        "   L'imputation par nombres arbitraire peut être utilisée lorsque les données manquentes ne sont pas de manière aléatoire, lorsque nous construisons des modèles non linéaires et lorsque le pourcentage de données manquantes est élevé. Cette technique d'imputation fausse la distribution des variables d'origine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB2Xt0kYcQGs"
      },
      "source": [
        "#Importez des pandas et les fonctions et classes requises depuis scikit-learn\n",
        "import pandas as pd\n",
        "\n",
        "# to split the data sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# to impute missing data with sklearn\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEBhODeZcZ08"
      },
      "source": [
        "# let's separate into training and testing set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtMJoichclOj"
      },
      "source": [
        "### Arbitrary imputation with pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjOTAObXcsBE"
      },
      "source": [
        "# Trouvez la valeur maximale pour quatres variables numériques :\n",
        "X_train[['A2','A3', 'A8', 'A11']].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2M8SHISc6hl"
      },
      "source": [
        "# Remplaçons les valeurs manquantes par 99 dans les variables numériques que nous avons spécifiées\n",
        "for var in ['A2','A3', 'A8', 'A11']:\n",
        "    X_train[var].fillna(99, inplace=True)\n",
        "    X_test[var].fillna(99, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgV4jTT5dO7d"
      },
      "source": [
        "Nous avons choisi 99 comme valeur arbitraire car elle est supérieure à la valeur maximale de ces variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8OB33IvdZ30"
      },
      "source": [
        "### Arbitrary imputation with Scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A8FvJ0Jdkks"
      },
      "source": [
        "# let's separate into training and testing set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data[['A2', 'A3', 'A8', 'A11']],\n",
        "    data['A16'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lykpJ0bdozX"
      },
      "source": [
        "\n",
        "# create an instance of the simple imputer\n",
        "imputer = SimpleImputer(strategy='constant', fill_value=99)\n",
        "\n",
        "# we fit the imputer to the train set\n",
        "imputer.fit(X_train)\n",
        "\n",
        "# we can look at the constant values:\n",
        "imputer.statistics_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bcv6fzNdyS1"
      },
      "source": [
        "Si votre ensemble de données contient des variables catégorielles, SimpleImputer() ajoutera 99 à ces variables également si des valeurs sont manquantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFdX73izd6cr"
      },
      "source": [
        "# and now we impute the train and test set\n",
        "# NOTE: the data is returned as a numpy array!!!\n",
        "\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGf6IbnGeTGA"
      },
      "source": [
        "### Arbitrary imputation imputation with Sklearn selecting features to impute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1fimsLUeVrg"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# to impute missing data with sklearn\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# to split the data sets\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJQwP4ZQeYqF"
      },
      "source": [
        "# let's separate into training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop('A16', axis=1),data['A16' ], test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJgkuopgebKt"
      },
      "source": [
        "# first we need to make a list with the numerical vars\n",
        "features_arbitrary = ['A2', 'A3', 'A8', 'A11']\n",
        "features_mean = ['A15']\n",
        "\n",
        "# then we instantiate the imputer within a pipeline\n",
        "arbitrary_imputer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value=99))])\n",
        "\n",
        "mean_imputer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean'))])\n",
        "\n",
        "# then we put the features list and the imputer in\n",
        "# the column transformer\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('arbitrary_imputer', arbitrary_imputer, features_arbitrary),\n",
        "    ('mean_imputer', mean_imputer, features_mean)\n",
        "    ], remainder='passthrough')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUaw5WJveeoP"
      },
      "source": [
        "\n",
        "# now we fit the preprocessor\n",
        "preprocessor.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dggrQUbhehY3"
      },
      "source": [
        "# and now we impute the data\n",
        "X_train = preprocessor.transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waxeoocQej6-"
      },
      "source": [
        "# Note that Scikit-Learn transformers return NumPy arrays!!\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HueDTaITfJbo"
      },
      "source": [
        "### Imputation par catégorie manquante\n",
        "Les données manquantes dans les variables catégorielles peuvent être traitées comme une catégorie différente. Dans cette recette, nous allons créer une catégorie « Manquante » pour remplacer les valeurs manquantes dans les variables catégorielles à l'aide de pandas, Scikit-learn. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLOXf76YfJg2"
      },
      "source": [
        "### Ajouter une catégorie avec des pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeHLMKS9gxHf"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlA7i5bRhBRm"
      },
      "source": [
        "# replace NA in some categorical variables\n",
        "\n",
        "for var in ['A4', 'A5', 'A6', 'A7']:\n",
        "\n",
        "    X_train[var].fillna('Missing', inplace=True)\n",
        "    X_test[var].fillna('Missing', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdbsxa4zhNkP"
      },
      "source": [
        "# check absence of missing values\n",
        "X_train[['A4', 'A5', 'A6', 'A7']].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMC-HshKhfVe"
      },
      "source": [
        "### Ajouter une catégorie 'Missing' avec Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBCrIDonhTwF"
      },
      "source": [
        "# let's separate into training and testing sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data[['A4', 'A5', 'A6', 'A7']], data['A16'], test_size=0.3, random_state=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buZiaffLhpl_"
      },
      "source": [
        "# create an instance of the simple imputer\n",
        "imputer = SimpleImputer(strategy='constant', fill_value='Missing')\n",
        "\n",
        "# we fit the imputer to the train set\n",
        "imputer.fit(X_train)\n",
        "\n",
        "# we can look at the new category:\n",
        "imputer.statistics_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lwbejGnh9E-"
      },
      "source": [
        "SimpleImputer() de scikit-learn remplacera les valeurs manquantes par Missing dans les variables numériques et catégorielles. Faites attention à ce comportement ou vous finirez par convertir accidentellement vos variables numériques en objets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJDAYs9-h-J3"
      },
      "source": [
        "\n",
        "# and now we impute the train and test set\n",
        "# NOTE: the data is returned as a numpy array!!!\n",
        "\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ84o5bPiDu-"
      },
      "source": [
        "\n",
        "pd.DataFrame(X_train).isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTPUQk3NkTxe"
      },
      "source": [
        "## Ajout d'une variable indicatrice manquante\n",
        "\n",
        "Un indicateur manquant est une variable binaire qui spécifie si une valeur manquait pour une observation (1) ou non (0). Il est courant de remplacer les observations manquantes par la moyenne, la médiane ou le mode tout en marquant ces observations manquantes avec un indicateur manquant, couvrant ainsi deux angles : si les données manquaient sont au hasard, cela serait envisagé par la moyenne, la médiane ou l'imputation par mode, et si ce n'était pas le cas, cela serait capturé par l'indicateur manquant. Dans cette recette, nous allons apprendre à ajouter des indicateurs manquants à l'aide de NumPy, scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX5VSbhWmgg1"
      },
      "source": [
        "#Commençons par importer les packages requis et préparer les données\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# to split the data sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import MissingIndicator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkM5A3oXnH-8"
      },
      "source": [
        "# let's separate into training and testing sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qw2izv4nOAc"
      },
      "source": [
        "### Ajout d'une variable indicatrice manquante avec pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKdOPHD_nVvL"
      },
      "source": [
        "# add missing indicator\n",
        "\n",
        "for var in ['A1', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8']:\n",
        "\n",
        "    X_train[var+'_NA'] = np.where(X_train[var].isnull(), 1, 0)\n",
        "    X_test[var+'_NA'] = np.where(X_test[var].isnull(), 1, 0)\n",
        "\n",
        "    \n",
        "# check the new missing indicator variables\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNEoTkFWoBpb"
      },
      "source": [
        "\n",
        "Notez comment nous nommons les nouveaux indicateurs manquants en utilisant le nom de variable d'origine, plus _NA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jfpu-GBoZE9"
      },
      "source": [
        "# the mean of the missing indicator should be the same as the \n",
        "# percentage of missing values in the original variable\n",
        "\n",
        "X_train['A3'].isnull().mean(), X_train['A3_NA'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgDI9vwroeuU"
      },
      "source": [
        "### Ajout d'une variable indicatrice manquante avec Scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLhigklcn_-z"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk18_PmZoxqx"
      },
      "source": [
        "indicator = MissingIndicator(error_on_new=True, features='missing-only')\n",
        "indicator.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE1TcMQ4o3zP"
      },
      "source": [
        "\n",
        "# we can see the features with na:\n",
        "# the result shows the column index in the NumPy array\n",
        "\n",
        "indicator.features_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qboa9PQDo92C"
      },
      "source": [
        "# with Sklearn we need to join the missing indicators dataframe\n",
        "# to the original X_train\n",
        "\n",
        "# let's create a column name for each of the new MissingIndicators\n",
        "indicator_cols = [c+'_NA' for c in X_train.columns[indicator.features_]]\n",
        "\n",
        "# and now let's concatenate the original dataset with the missing indicators\n",
        "X_train = pd.concat([\n",
        "    X_train.reset_index(),\n",
        "    pd.DataFrame(indicator.transform(X_train), columns = indicator_cols)],\n",
        "    axis=1)\n",
        "\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3kE-2USoAZQ"
      },
      "source": [
        "## Effectuer une imputation multivariée avec MICE, KNN ou Random Forests\n",
        "\n",
        "Les méthodes d'imputation multivariée, par opposition à l'imputation univariée, utilisent l'ensemble des variables pour estimer les valeurs manquantes. En d'autres termes, les valeurs manquantes d'une variable sont modélisées en fonction des autres variables de l'ensemble de données. L'imputation multivariée par (MICE)(**Multivariate imputation by chainedequations**) est une technique d'imputation multiple qui modélise chaque variable avec des valeurs manquantes en fonction des variables restantes et utilise cette estimation pour l'imputation MICE a les étapes de base suivantes\n",
        "1. Une imputation univariée simple est effectuée pour chaque variable avec des données manquantes, par exemple, l'imputation médiane\n",
        "2. Une variable spécifique est sélectionnée, disons, var_1, et les valeurs manquantes sont redéfinies comme manquantes.\n",
        "3. Un modèle utilisé pour prédire var_1 est construit en fonction des variables restantes dans l'ensemble de données.\n",
        "4. Les valeurs manquantes de var_1 sont remplacées par les nouvelles estimations\n",
        "5. Répétez les étapes 2 à 4 pour chacune des variables restantes.\n",
        "\n",
        "Une fois que toutes les variables ont été modélisées sur la base du reste, un cycle d'imputation est conclu. Les étapes 2 à 4 sont effectuées plusieurs fois, généralement 10 fois, et les valeurs d'imputation après chaque tour sont conservées. L'idée est qu'à la fin des cycles, la distribution des paramètres d'imputation devrait avoir convergé\n",
        "\n",
        "Chaque variable avec des données manquantes peut être modélisée en fonction de la variable restante en utilisant plusieurs approches, par exemple, la régression linéaire, les baies, les arbres de décision, les k plus proches voisins et les forêts aléatoires\n",
        "\n",
        "Dans cette recette, nous allons implémenter MICE en utilisant scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCrQQkGUszHs"
      },
      "source": [
        "# Importons les bibliothèques et classes Python requises\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNQFcD2jtE9m"
      },
      "source": [
        "#Chargeons l'ensemble de données avec quelques variables numériques :\n",
        "variables = ['A2','A3','A8', 'A11', 'A14', 'A15', 'A16']\n",
        "data = data[variables]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COwswAgwuSNX"
      },
      "source": [
        "Les modèles qui seront utilisés pour estimer les valeurs manquantes doivent être construits sur les données du train et utilisés pour imputer des valeurs dans le train, le test et les données futures :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9YWZacRuDaw"
      },
      "source": [
        "# let's separate into training and testing set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P85S071kucK6"
      },
      "source": [
        "\n",
        "# let's create a MICE imputer using Bayes as estimator\n",
        "imputer = IterativeImputer(estimator= BayesianRidge(),\n",
        "                           max_iter=10, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6-JFjlgusBc"
      },
      "source": [
        "IterativeImputer() contient d'autres arguments utiles. Par exemple, nous pouvons spécifier la première stratégie d'imputation à l'aide du paramètre initial_strategy et spécifier comment nous voulons faire défiler les variables soit de manière aléatoire, soit de celle avec le moins de valeurs manquantes à celle avec le plus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RegOMWigus1v"
      },
      "source": [
        "imputer.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STIPy9Rxu1op"
      },
      "source": [
        "# transform the data - replace the missing values\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ZbBjdKvhxF"
      },
      "source": [
        "\n",
        "En utilisant IterativeImputer() de scikit-learn, nous pouvons modéliser des variables à l'aide de plusieurs algorithmes, tels que Bayes, les k-voisins les plus proches, les arbres de décision et les forêts aléatoires. Effectuez les étapes suivantes pour ce faire :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36-O_D6svmSm"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PM09ZipvqCi"
      },
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0xS76qevumv"
      },
      "source": [
        "#Construire des imputeurs MICE en utilisant différentes stratégies de modélisation\n",
        "imputer_bayes = IterativeImputer(estimator=BayesianRidge(),\n",
        "                                 max_iter=10,\n",
        "                                 random_state=0)\n",
        "\n",
        "imputer_knn = IterativeImputer(estimator=KNeighborsRegressor(n_neighbors=5),\n",
        "                                 max_iter=10,\n",
        "                                 random_state=0)\n",
        "\n",
        "imputer_nonLin = IterativeImputer(estimator=DecisionTreeRegressor(max_features='sqrt', random_state=0),\n",
        "                                 max_iter=10,\n",
        "                                 random_state=0)\n",
        "\n",
        "imputer_missForest = IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=10, random_state=0),\n",
        "                                 max_iter=10,\n",
        "                                 random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OrMSEPSwmXv"
      },
      "source": [
        "Notez comment, dans le bloc de code précédent, nous créons quatre imputeurs MICE différents, chacun avec un algorithme d'apprentissage automatique différent qui sera utilisé pour modéliser chaque variable en fonction des variables restantes dans l'ensemble de données\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOwN3ydKvxfA"
      },
      "source": [
        "#Fit the MICE imputers to the train set\n",
        "imputer_bayes.fit(X_train)\n",
        "imputer_knn.fit(X_train)\n",
        "imputer_nonLin.fit(X_train)\n",
        "imputer_missForest.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peTZPa0bv1Kf"
      },
      "source": [
        "#Impute missing values in the train set\n",
        "X_train_bayes = imputer_bayes.transform(X_train)\n",
        "X_train_knn = imputer_knn.transform(X_train)\n",
        "X_train_nonLin = imputer_nonLin.transform(X_train)\n",
        "X_train_missForest = imputer_missForest.transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "airIepIcv36B"
      },
      "source": [
        "predictors = [var for var in variables if var !='A16']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD-ChTkXxJD3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub44HNlAv7Mw"
      },
      "source": [
        "# Convert the NumPy arrays into dataframes\n",
        "X_train_bayes = pd.DataFrame(X_train_bayes, columns = predictors)\n",
        "X_train_knn = pd.DataFrame(X_train_knn, columns = predictors)\n",
        "X_train_nonLin = pd.DataFrame(X_train_nonLin, columns = predictors)\n",
        "X_train_missForest = pd.DataFrame(X_train_missForest, columns = predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPWYwdivwDWE"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "X_train['A3'].plot(kind='kde', ax=ax, color='blue')\n",
        "X_train_bayes['A3'].plot(kind='kde', ax=ax, color='green')\n",
        "X_train_knn['A3'].plot(kind='kde', ax=ax, color='red')\n",
        "X_train_nonLin['A3'].plot(kind='kde', ax=ax, color='black')\n",
        "X_train_missForest['A3'].plot(kind='kde', ax=ax, color='orange')\n",
        "\n",
        "# add legends\n",
        "lines, labels = ax.get_legend_handles_labels()\n",
        "labels = ['A3 original', 'A3 bayes', 'A3 knn', 'A3 Trees', 'A3 missForest']\n",
        "ax.legend(lines, labels, loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHiOyOqExsTv"
      },
      "source": [
        "Dans le graphique précédent, nous pouvons voir que les différents algorithmes renvoient des distributions légèrement différentes de la variable d'origine"
      ]
    }
  ]
}